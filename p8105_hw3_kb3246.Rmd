---
title: "Data Science HW-3"
author: "Kasturi Bhamidipati"
date: "2022-10-13"
output: github_document
---

First we will load the packages that we need! 

```{r loading packages and setup}
library(tidyverse)
library(ggridges)
library(skimr)
library(patchwork)
library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```
# Problem 1 

#### Read in the data

```{r}
data("instacart")

instacart = 
  instacart %>% 
  as_tibble(instacart)
```

#### Answer questions about the data

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns, with each row resprenting a single product from an instacart order. Variables include identifiers for user, order, and product; the order in which each product was added to the cart. There are several order-level variables, describing the day and time of the order, and number of days since prior order. Then there are several item-specific variables, describing the product name (e.g. Yogurt, Avocado), department (e.g. dairy and eggs, produce), and aisle (e.g. yogurt, fresh fruits), and whether the item has been ordered by this user in the past. In total, there are `r instacart %>% select(product_id) %>% distinct %>% count` products found in `r instacart %>% select(user_id, order_id) %>% distinct %>% count` orders from `r instacart %>% select(user_id) %>% distinct %>% count` distinct users.

Below is a table summarizing the number of items ordered from aisle. In total, there are 134 aisles, with fresh vegetables and fresh fruits holding the most items ordered by far.

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

Next is a plot that shows the number of items ordered in each aisle. Here, aisles are ordered by ascending number of items.

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))

```

Our next table shows the three most popular items in aisles `baking ingredients`, `dog food care`, and `packaged vegetables fruits`, and includes the number of times each item is ordered in your table.

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```

Finally is a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week. This table has been formatted in an untidy manner for human readers. Pink Lady Apples are generally purchased slightly earlier in the day than Coffee Ice Cream, with the exception of day 5.

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)
```


# Problem 2 
We need to load and tidy the dataset. 

```{r Accel_data Tidying}
accel_data = 
  read_csv(
    "data/accel_data.csv")%>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute", 
    names_prefix = "activity_",
    values_to = "activity"
    ) %>% 
  mutate( minute = as.numeric(minute),
          day_of_the_week = 
            recode(day,
                 "Monday" = "Weekday", 
                 "Tuesday" = "Weekday",
                 "Wednesday" = "Weekday", 
                 "Thursday" = "Weekday", 
                 "Friday"= "Weekday", 
                 "Saturday" = "Weekend", 
                 "Sunday"= "Weekend"
                   )
  )
```

### Summary of the `accel_data` dataset

This is the `accel_data` dataset.

`r skim(accel_data)`

- It consists of `r nrow(accel_data)` rows and `r ncol(accel_data)` columns. 

- The dataset gives us information on the the day of the exercise, the minute of the day, the activity count, a unique `day_ID` variable for each day, a variable categorizing the days into either weekdays or weekends and the week number. 

### Total Acitivity - Traditional Analysis

Here we have the code chunk to calculate total activity per day, across the different weeks. 
```{r Total Activity}
traditional_anal = 
  accel_data %>% 
  group_by(week, day) %>%
  summarise(total_activity = sum(activity)) %>%
  pivot_wider(
    names_from = day, 
    values_from = total_activity
  ) %>%
  select("week","Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")
```

Yes, there are trends that are apparent. I notice that, on average, the total activity during the weekdays is more than that of the weekends. I also find some irregular/potentially wrong data values because in weeks 4 and 5, the total activity on Saturday is extremely low, which ideally should not be the case. 

### Single Panel Plot

```{r single panel plot, warning=FALSE}
  accel_data %>%
  ggplot(aes(x = minute/60, y = activity, color = day))+
  geom_line(alpha = .30)+ 
  geom_point(alpha = .30)+ 
  geom_smooth( aes(group = day,se = FALSE))+
  labs(
    title = "24-hour Activity Time Courses For Each Day", 
    x = "Hour", 
    y = "Activity"
  )+
  theme_bw()
```

What do we say about this????? 

# Problem 3

First we want to load the package. 

```{r importing ny_noaa}
data("ny_noaa")
```

### Summary of `ny_noaa` dataset

This is the `ny_noaa` dataset. 

`r skim(ny_noaa)`

- It has `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. 

- The dataset gives us information on precipitation, snow fall, snow depth, maximum and minimum temperatures on a specific day, in New York State. It also gives us information on the weather station ID through which the data was collected. 

- It includes the years 1981 to 2010. 

### Cleaning the dataset 

```{r cleaning ny_noaa}
nys_weather = 
  ny_noaa %>%
  janitor::clean_names() %>%
  separate(date, into = c("year", "month", "day"), sep = "-", convert = TRUE)%>%
  mutate(month = month.abb[month], 
         prcp = prcp/10, 
         tmax = as.numeric(tmax),
         tmin = as.numeric(tmin)
  ) %>% 
  select(id, day, month, year, everything()) %>% 
  mutate(
    tmax = tmax/10,
    tmin = tmin/10
    )
```

The most commonly observed values for snowfall are 0s. I assume this is because on most days there was no snowfall recorded. It also makes sense because it does not snow for a large portion of the year, and so the days where we actually have snowfall make up a very small portion of the data in comparison to no snowfall days. 

### Two-Panel Plot - Average Max Temp  

```{r average max temp}
max_temp= 
  nys_weather %>%
  group_by(id, month, year) %>%
  filter(month %in% c("Jan","Jul")) %>%
  summarise(tmax_avg = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = tmax_avg, colour = id))+
    geom_line(alpha=.25)+
    theme(legend.position = "none", axis.text.x = element_text(angle = 60, hjust = 1))+
    facet_grid(.~ month)+
    labs(
    title = "Average max Temperature in January and July (C)",
    x = "Year",
    y = "Average Max Temperature (C)"
    )
  
max_temp

```

From the plots, I observe that there are lower average maximum temperature in January than in July. 
However, we do also have some outliers that I can notice from this graph. This could be because some of the weather stations reporter unusually lower/higher average maximum temperatures at some point during the years that were included in this dataset. 

### Two-Panel Plot - `tmax` vs. `tmin` and Snowfall 

`
```{r max/min and snow fall}
tmax_tmin_p = 
  nys_weather %>% 
  ggplot(aes(x = tmax, y = tmin))+
  geom_hex()+
  theme(legend.position = "none")+
  labs(
    title = "tmax vs. tmin",
    x = "tmax (C)",
    y = "tmin (C)"
    )
  

snowfall= 
  nys_weather %>% 
  group_by(year) %>%
  filter(snow %in% (0:100)) %>%
  ggplot(aes(x = snow, y = as.factor(year))) + 
  geom_density_ridges()+
  theme(legend.position = "none")+ 
  labs(
    title = "Distribution of Snowfall(mm) from 1981-2010 in NYS",
    x = "Snowfall (mm)",
    y = "Year"
    )

snowfall

tmax_tmin_p + snowfall
```




